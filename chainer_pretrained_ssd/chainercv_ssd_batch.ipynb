{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chainerの学習済み物体検出モデルを利用した推論\n",
    "\n",
    "## 概要\n",
    "このノートブックでは、Chainerの学習済み物体検出モデルを利用して以下の2種類の推論を行います。\n",
    "\n",
    "- 推論エンドポイントにモデルをホストして、リアルタイムに推論を実行する。\n",
    "- リクエストを受けたときだけ、エンドポイントを起動し、S3にあるファイルを一括で推論する（バッチ変換ジョブ）。\n",
    "\n",
    "物体検出のアルゴリズムはいくつかありますが、ここではSSD (Single Shot Multibox Detector) を利用します。Chainer公式の学習済みモデルは以下からダウンロードすることができます。\n",
    "\n",
    "https://github.com/chainer/chainercv/tree/master/examples/ssd\n",
    "\n",
    "### (注意点)\n",
    "ChainerCVでは、特定のDeep Neural Networks(SSD含む)を構築する際に、学習済みモデルをロードすることができます。従って、*物体検出モデルの事前ダウンロードは不要*です。しかし今回は、学習済みモデルをSageMakerに取り込む方法を一通り体験するために、事前にダウンロードして、別途ロードするという手順を行います。\n",
    "\n",
    "## 学習済みモデルのダウンロードとS3へのアップロード\n",
    "\n",
    "上記のURLでは、SSD300とSSD512のモデルが提供されており、今回はSSD300を利用します。300や512は入力画像のサイズを表します。一般に、画像サイズの大きい512のほうが精度が良いですが、推論に多くの計算を必要とします。\n",
    "\n",
    "SSD300のモデルをダウンロードするために以下を実行します。ダウンロードされたモデルは、このノートブックインスタンスの`/tmp/ssd_model.npz`に保存されます。SageMakerでモデルをホスティングするためには、tar.gz形式にしてS3にアップロードする必要があります。`model.tar.gz`に変換した後、SageMaker Python SDKの`upload_data`を利用してS3にアップロードします。アップロードされる先は、`s3://sagemaker-{リージョン名}-{12桁アカウントID}/notebook/chainercv_ssd/model.tar.gz`になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "# Setup\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# This role retrieves the SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()\n",
    "\n",
    "# Download the model weights.\n",
    "try:\n",
    "    url = 'https://chainercv-models.preferred.jp/ssd300_voc0712_trained_2017_08_08.npz'\n",
    "    urllib.request.urlretrieve (url, '/tmp/ssd_model.npz')\n",
    "\n",
    "# Tar and compress the model.\n",
    "    with tarfile.open('/tmp/model.tar.gz', \"w:gz\") as tar:\n",
    "         tar.add('/tmp/ssd_model.npz', arcname='ssd_model.npz')\n",
    "\n",
    "# Upload the model. The `ChainerModel` will use `uploaded_data` to download this model.\n",
    "\n",
    "    uploaded_model = sagemaker_session.upload_data(path='/tmp/model.tar.gz', \n",
    "                                                   key_prefix='notebook/chainercv_ssd')\n",
    "finally:\n",
    "    os.remove('/tmp/model.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ホスティング用のスクリプトの作成\n",
    "\n",
    "SageMakerでホスティングするためには、モデルの利用方法（どういうモデルを読み込むか、前処理をいれるかなど）を決めるPythonスクリプトが必要になります。Chainerの場合は、シリアライズされたモデルはネットワークの重みだけを含んでいるため、シンボルを定義して、そこに重みの値をロードするという処理が必要になります。\n",
    "\n",
    "同梱されている`chainercv_ssd.py`が、そのためのスクリプトです。`chainercv_ssd.py`には、`model_fn`、`input_fn`、`predict_fn`の関数が定義されていることがわかります。以下は、ホスティングのために実装する関数の一覧です。このうち `model_fn` は必ず実装しなければなりません。\n",
    "\n",
    "\n",
    "### ホスティング用関数一覧\n",
    "\n",
    "* **`model_fn(model_dir)`**: この関数は`model_dir`に保存されているモデルをロードする関数です。上述したように、シンボルの定義を行ってからロードします。シンボルの定義にはChainerCVを利用します。\n",
    "\n",
    "* **`input_fn(input_data, content_type)`**: この関数は、推論リクエストを受け付けたときに、推論用のデータ`input_data`に対する前処理を書く関数です。`content_type`を同時に受け取ることができるので、`content_type`に応じて条件分岐を作成し、異なる前処理を実装することができます。numpy形式`application/x-npy`を受け取る関数が標準実装されています。jpegなどのバイナリを受け取る場合は、その処理を追加実装する必要があります。\n",
    "  \n",
    "* **`predict_fn(input_data, model)`**: この関数は `input_fn` で前処理されてreturnされた値を`input_data`として受け取り、`model_fn`でロードした`model`で推論するコードを書く関数です。 \n",
    "  \n",
    "* **`output_fn(prediction, accept)`**: この関数は `predict_fn`のreturnした値`prediction`を後処理するための関数です。`accept`に応じて処理を変更することもできます。\n",
    "\n",
    "### 関数の流れ\n",
    "上記では文章で書きましたが、擬似的なコードで示すと、関数の実行順はこのような流れになります。\n",
    "```python\n",
    "# Load a model from file system\n",
    "model = model_fn(model_dir)\n",
    "\n",
    "# Deserialize the Invoke request body into an object we can perform prediction on\n",
    "input_object = input_fn(request_body, request_content_type)\n",
    "\n",
    "# Perform prediction on the deserialized object, with the loaded model\n",
    "prediction = predict_fn(input_object, model)\n",
    "\n",
    "# Serialize the prediction result into the desired response content type\n",
    "output = output_fn(prediction, response_content_type)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルをホスティング\n",
    "\n",
    "S3にモデルをアップロードして、ホスティング用のスクリプトがそろったら、SageMakerにそのモデルを登録します。Chainerの場合は`ChainerModel`を利用します。このとき、アップロードしたモデルと、スクリプトを指定する必要があります。もしモデルを登録したら`deploy`でエンドポイントを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.chainer.model import ChainerModel\n",
    "from sagemaker.utils import sagemaker_timestamp\n",
    "\n",
    "model = ChainerModel(model_data=uploaded_model, role=role, entry_point='chainercv_ssd.py')\n",
    "\n",
    "endpoint_name = 'chainer-ssd-{}'.format(sagemaker_timestamp())\n",
    "\n",
    "predictor = model.deploy(instance_type='ml.m4.xlarge', initial_instance_count=1, endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ホストしたモデルに対して推論リクエストを送信\n",
    "\n",
    "モデルをホストできたら`predictor`を利用して画像を送ります。`predictor.predict(image)`でリクエストを送ることができます。結果が、`predict_fn`で定義したように、bounding box, label, score がdict形式で返ってくることを確認しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainercv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plot\n",
    "\n",
    "image = chainercv.utils.read_image('images/car1.jpg', color=True)\n",
    "image = np.ascontiguousarray(image, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.predict(image).tolist()\n",
    "bbox = result['bbox']\n",
    "label = result['label']\n",
    "score = result['score']\n",
    "\n",
    "print('bounding box: {}\\nlabel: {}\\nscore: {}'.format(bbox, label, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChainerCVを利用すると、簡単に画像の上にbounding box、label、scoreをのせることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from chainercv.visualizations import vis_bbox\n",
    "from chainercv.datasets import voc_bbox_label_names\n",
    "import matplotlib.pyplot as plt\n",
    "vis_bbox(image, bbox, label, score, label_names=voc_bbox_label_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3にあるファイルを一括で推論する（バッチ変換ジョブ）\n",
    "\n",
    "エンドポイントを作成すると、リクエストに対してすぐに結果を返すことができます。一方で、エンドポイント実行中は常にコストがかかります。例えば、画像をS3に保存しておき、1日に1回まとめて推論を行う場合には、推論したいときだけエンドポイントを利用し、それ以外のときはエンドポイントを停止するほうが効率的です。それを実現するジョブが、バッチ変換ジョブです。\n",
    "\n",
    "### バッチ変換ジョブの流れ\n",
    "\n",
    "バッチ変換ジョブをリクエストすると以下のような処理がはしります。\n",
    "\n",
    "1. エンドポイントを起動\n",
    "1. 指定されたS3のファイル群をエンドポイントに送信\n",
    "1. 推論結果をS3に保存\n",
    "1. すべての推論が終わるとエンドポイントを削除\n",
    "\n",
    "エンドポイントの削除まで自動で行われるため、コストを抑えながら、一括でデータを処理するのに向いています。\n",
    "バッチ変換ジョブ用に`images`フォルダにおいている画像をS3にアップロードしましょう。アップロード先は\n",
    "`s3://sagemaker-{リージョン名}-{12桁アカウントID}/batch_transform/ssd`です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_set = sagemaker_session.upload_data(path='./images', key_prefix='batch_transform/ssd')\n",
    "print(\"Images are uploaded to {}\".format(image_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## バッチ変換ジョブの実行\n",
    "\n",
    "バッチ変換ジョブを実行するためには、推論に利用するモデル`model`から`transformer`を作成します。`transoformer`に対して、アップロードした画像のS3のパスを渡して、バッチ変換ジョブを実行します。 \n",
    "\n",
    "バッチ変換ジョブは大量のデータを一括処理するために用いられるので、実行してもノートブック側でその終了を待たない仕様になっています。ここでは、ジョブの終了がわかりやすいように、`tranformer.wait()`でジョブの終了を待ちます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = model.transformer(instance_count=1,\n",
    "                                output_path='s3://' +sagemaker_session.default_bucket() + '/batch_transform/output',\n",
    "                                instance_type='ml.m4.xlarge')\n",
    "transformer.transform(image_set, content_type='image/jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 出力結果をダウンロードして確認\n",
    "\n",
    "出力先のS3パスが`transformer.output_path`に記録されているので、ここからファイルをダウンロードします。ダウンロードには、AWSのPython SDKである`boto3`を使います。出力ファイル名は`入力ファイル名.out`です。例えば、`car1.jpg`の出力結果は`car1.jpg.out`になります。ファイルの中身は`predict_fn`で定義した内容になります。\n",
    "\n",
    "### 出力結果のダウンロード\n",
    "\n",
    "ディレクトリ`output`に推論結果をダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import boto3\n",
    "\n",
    "parsed_url = urlparse(transformer.output_path)\n",
    "bucket_name = parsed_url.netloc\n",
    "prefix = parsed_url.path[1:]\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "bucket = s3_resource.Bucket(bucket_name)\n",
    "\n",
    "output_dir = 'output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for key in bucket.objects.filter(Prefix = prefix):\n",
    "    filename = os.path.basename(key.key)\n",
    "    print(\"File {} is downloaded to {}\".format(filename,output_dir))\n",
    "    bucket.download_file(key.key, output_dir + \"/\" + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 出力結果の確認\n",
    "\n",
    "ダウンロードした推論結果をパースして、画像の上に結果を出力します。dict形式で出力したので、`json.load`でファイルを読み込むことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from chainercv.visualizations import vis_bbox\n",
    "from chainercv.datasets import voc_bbox_label_names\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file_names = []\n",
    "bbox_list = []\n",
    "label_list = []\n",
    "score_list = []\n",
    "\n",
    "import json\n",
    "for f in os.listdir(output_dir):\n",
    "    if \"out\" in f:\n",
    "        file_names.append(os.path.splitext(f)) \n",
    "        f = open(\"output/\" + f, 'r')\n",
    "        result = json.load(f)\n",
    "        bbox_list.append(result['bbox'])\n",
    "        label_list.append(result['label'])\n",
    "        score_list.append(result['score'])\n",
    "\n",
    "for i in range(len(file_names)):\n",
    "    image = chainercv.utils.read_image(\"images/\" + file_names[i][0], color=True)\n",
    "    image = np.ascontiguousarray(image, dtype=np.uint8)\n",
    "    vis_bbox(image, bbox_list[i], label_list[i], score_list[i], label_names=voc_bbox_label_names)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エンドポイントの削除\n",
    "\n",
    "エンドポイントを起動しているとコストがかかり続けます。不要な場合は`delete_endpoint()`を呼び出して削除します。バッチ変換ジョブは、ジョブ終了に伴って自動でエンドポイントが削除されるため、ここで削除するのは、最初に`deploy`で作成したエンドポイントのみです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_chainer_p36",
   "language": "python",
   "name": "conda_chainer_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific  language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
